{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Computing TF-IDF Vectors with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About dataset source\n",
    "\n",
    "Scikit-Learn provides us with data from Usenent, which is a well-established online collection of discussion forums. These Usenent forums are called newsgroups. Each individual newsgroup focuses on some topic of discussion. That discussion topic is briefly outlined within the newsgroup name. Users within a newsgroup converse by posting messages. These user posts are generally not limited in length. Some of the posts can get quite large. Both the diversity and the varying lengths of the posts will give us a chance to expand our NLP skills. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(remove=('headers','footers'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newsgroups object contains posts from 20 different newsgroups. Each newsgroup’s discussion-theme is outlined in its names. We can view these newsgroup names by printing newsgroups.target_names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, comp.sys.mac.hardware focuses on mac hardware. Meanwhile, comp.sys.ibm.pc_hardware focuses on pc hardware. Categorically, these 2 newsgroups are exceedingly similar. Their only differentiator is whether the computer hardware belongs to a mac or pc. Sometimes, categorical differences are subtle. Boundaries between text topics are fluid, and are not necessarily etched in stone. We’ll need to keep this in mind, later in the section, when we proceed to cluster the newsgroup posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get newsgroup group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "\n",
      "The post at index 1 first appeared in the 'comp.sys.mac.hardware' group\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "print(newsgroups.data[index])\n",
    "#get newsgroup\n",
    "origin = newsgroups.target_names[newsgroups.target[index]]\n",
    "print(f\"\\nThe post at index {index} first appeared in the '{origin}' group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get newsgroup data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our dataset contains 11314 newsgroup posts\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(newsgroups.data)\n",
    "print(f\"Our dataset contains {dataset_size} newsgroup posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains over 11,000 posts. **Our goal is to cluster these posts by topic.**\n",
    "Carrying out text clustering on this scale will require computational efficiency. We’ll need to efficiently compute newsgroup-post similarities by representing our text-data as a matrix. To do so, we’ll need to transform each newsgroup post into TF vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing Documents Using Scikit-Learn\n",
    "\n",
    "Scikit-Learn provides built-in class for transforming input texts into TF vectors. That class is called CountVectorizer. Initializing CountVectorizer will create a vectorizer object capable of vectorizing our texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need to do is run vectorizer.fit_transform(newsgroups.data). The method-call will return the TF matrix corresponding to the vectorized newsgroup posts. As a reminder, a TF matrix stores the counts of words (columns) across all texts (rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 108644)\t4\n",
      "  (0, 110106)\t1\n",
      "  (0, 57577)\t2\n",
      "  (0, 24398)\t2\n",
      "  (0, 79534)\t1\n",
      "  (0, 100942)\t1\n",
      "  (0, 37154)\t1\n",
      "  (0, 45141)\t1\n",
      "  (0, 70570)\t1\n",
      "  (0, 78701)\t2\n",
      "  (0, 101084)\t4\n",
      "  (0, 32499)\t4\n",
      "  (0, 92157)\t1\n",
      "  (0, 100827)\t6\n",
      "  (0, 79461)\t1\n",
      "  (0, 39275)\t1\n",
      "  (0, 60326)\t2\n",
      "  (0, 42332)\t1\n",
      "  (0, 96432)\t1\n",
      "  (0, 67137)\t1\n",
      "  (0, 101732)\t1\n",
      "  (0, 27703)\t1\n",
      "  (0, 49871)\t2\n",
      "  (0, 65338)\t1\n",
      "  (0, 14106)\t1\n",
      "  :\t:\n",
      "  (11313, 55901)\t1\n",
      "  (11313, 93448)\t1\n",
      "  (11313, 97535)\t1\n",
      "  (11313, 93393)\t1\n",
      "  (11313, 109366)\t1\n",
      "  (11313, 102215)\t1\n",
      "  (11313, 29148)\t1\n",
      "  (11313, 26901)\t1\n",
      "  (11313, 94401)\t1\n",
      "  (11313, 89686)\t1\n",
      "  (11313, 80827)\t1\n",
      "  (11313, 72219)\t1\n",
      "  (11313, 32984)\t1\n",
      "  (11313, 82912)\t1\n",
      "  (11313, 99934)\t1\n",
      "  (11313, 96505)\t1\n",
      "  (11313, 72102)\t1\n",
      "  (11313, 32981)\t1\n",
      "  (11313, 82692)\t1\n",
      "  (11313, 101854)\t1\n",
      "  (11313, 66399)\t1\n",
      "  (11313, 63405)\t1\n",
      "  (11313, 61366)\t1\n",
      "  (11313, 7462)\t1\n",
      "  (11313, 109600)\t1\n"
     ]
    }
   ],
   "source": [
    "tf_matrix = vectorizer.fit_transform(newsgroups.data)\n",
    "print(tf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our printed tf_matrix does not appear to be a NumPy array. What sort of data structure is it? We can check, by printing type(tf_matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tf_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About csr matrix \n",
    "\n",
    " stands for Compressed Sparse Row, which is a storage format for compressing matrices that are composed mostly of zeros. **These mostly empty matrices are referred to as sparse matrices. They can be made smaller by storing only the non-zero elements. This compression leads to more efficient memory usage, and also faster computation. Large-scale text-based matrices are usually very sparse, since a single document normally contains just a small percentage of the total vocabulary**. Thus, Scikit-Learn automatically converts the vectorized text to the CSR format. The conversion is carried out using a csr_matrix class that’s imported from SciPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to minimize confusion between csr_matrix and numpy we convert matrix to numpy array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "tf_np_matrix = tf_matrix.toarray()\n",
    "print(tf_np_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rows - representation of the post\n",
    "* Cols - represent individual words\n",
    "* Vocab_size = cols "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our collection of 11314 newsgroup posts contain a total of 114751 unique words\n"
     ]
    }
   ],
   "source": [
    "assert tf_np_matrix.shape == tf_matrix.shape\n",
    "num_posts, vocabulary_size = tf_np_matrix.shape\n",
    "print(f\"Our collection of {num_posts} newsgroup posts contain a total of \"\n",
    "      f\"{vocabulary_size} unique words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data contains 114,000 unique words. However, most posts will hold only a few dozen of these words. **We can measure the unique word-count of a post at index i by counting the number of non-zero elements in row tf_np_matrix[i].** The easiest way to count these non-zero elements is with NumPy. The library allows us to obtain all non-zero indices of the vector at tf_np_matrix[i]. We simply need to input the vector into the np.flatnonzero function. Below, we’ll count and output the non-zero indices of the car post in newsgroups.data[0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The newsgroup in row 0 contains 64 unique words.\n",
      "The actual word-counts map to the following column indices:\n",
      "\n",
      "[ 14106  15549  22088  23323  24398  27703  29357  30093  30629  32194\n",
      "  32305  32499  37154  39275  42332  42333  43643  45089  45141  49871\n",
      "  49881  50165  54442  55453  57577  58321  58842  60116  60326  64083\n",
      "  65338  67137  67140  68931  69080  70570  72915  75280  78264  78701\n",
      "  79055  79461  79534  82759  84398  87690  89161  92157  93304  95225\n",
      "  96145  96432 100406 100827 100942 101084 101732 108644 109086 109254\n",
      " 109294 110106 112936 113262]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "row_n = 0\n",
    "tf_vector = tf_np_matrix[row_n]\n",
    "non_zero_indices = np.flatnonzero(tf_vector)\n",
    "num_unique_words = non_zero_indices.size\n",
    "print(f\"The newsgroup in row {row_n} contains {num_unique_words} unique words.\")\n",
    "print(\"The actual word-counts map to the following column indices:\\n\")\n",
    "print(non_zero_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Okay cool... but what are these words ? \n",
    "\n",
    "In order to find out, we’ll need a mapping between TF vector indices and word-values. That mapping can be generated by calling vectorizer.get_feature_names(). The method-call will return a list of words, which we’ll call words. Each index i will correspond to the ith word within the list. Thus, running [words[i] for i in non_zero_indices] will return all unique words within our post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['60s', '70s', 'addition', 'all', 'anyone', 'be', 'body', 'bricklin', 'bumper', 'called', 'can', 'car', 'could', 'day', 'door', 'doors', 'early', 'engine', 'enlighten', 'from', 'front', 'funky', 'have', 'history', 'if', 'in', 'info', 'is', 'it', 'know', 'late', 'looked', 'looking', 'made', 'mail', 'me', 'model', 'name', 'of', 'on', 'or', 'other', 'out', 'please', 'production', 'really', 'rest', 'saw', 'separate', 'small', 'specs', 'sports', 'tellme', 'the', 'there', 'this', 'to', 'was', 'were', 'whatever', 'where', 'wondering', 'years', 'you']\n"
     ]
    }
   ],
   "source": [
    "words = vectorizer.get_feature_names()\n",
    "unique_words = [words[i] for i in non_zero_indices]\n",
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve printed all the words in newsgroups.data[0]. **Of course, not all these words have equal mention-counts. Some words occur more frequently than others. Perhaps these frequent words are more relevant to the topic of cars.** Lets print the 10 most frequent words within the post, along with their associated counts. We’ll represent this output as a Pandas table, for visualization purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Non-Zero Elements of 1D NumPy Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* non_zero_indices = np.flatnonzero(np_vector): Returns the non-zero indices in a 1D NumPy array.\n",
    "* non_zero_vector = np_vector[non_zero_indices]: Selects the non-zero elements of a 1D NumPy array (assuming non_zero_indices corresponds to non-zero indices of that array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Data\":unique_words,\n",
    "    \"Counts\":tf_vector[non_zero_indices]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>the</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>this</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>was</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>car</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>if</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>is</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>it</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>from</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>on</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anyone</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Data  Counts\n",
       "53     the       6\n",
       "55    this       4\n",
       "57     was       4\n",
       "11     car       4\n",
       "24      if       2\n",
       "27      is       2\n",
       "28      it       2\n",
       "19    from       2\n",
       "39      on       2\n",
       "4   anyone       2"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data).sort_values(\"Counts\", ascending=False)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whats a noise..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three first words, have nothing to do with cars. These words, the, this, and was, are among the most common words in the English language. They don’t provide a differentiating signal between the car-post and a differently-themed post.Instead, the common words are a source of noise. They increase the likelihood that 2 unrelated documents will cluster together.\n",
    "\n",
    "NLP practitioners refer to such noisy words as **stop words**,because these words are blocked from appearing in the vectorized results. Stop words are generally deleted from the text prior to vectorization. T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we’ll re-initialize a stop-word aware vectorizer. Afterwards, we’ll rerun fit_transfrom in order to re-compute the TF matrix. The number of word-columns in that matrix will be less than our previously computed vocabulary size of 114,751. Also, we’ll regenerate our words list. Common stop words such as the, this, of and it will be missing from that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "tf_matrix = vectorizer.fit_transform(newsgroups.data)\n",
    "assert tf_matrix.shape[1] < 114751\n",
    "\n",
    "words = vectorizer.get_feature_names()\n",
    "for common_word in ['the', 'this', 'was', 'if', 'it', 'on']:\n",
    "    assert common_word not in words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All stop words have been deleted from the recomputed tf_matrix. Now, we can re-generate the 10 most frequent words in newsgroups.data[0]. Please note that in the process, we’ll recompute tf_np_matrix, tf_vector, unique_words, non_zero_indices, and df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top words after stop-word deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After stop-word deletion, 34 unique words remain.\n",
      "The 10 most frequent words are:\n",
      "\n",
      "       Word  Count\n",
      "        car      4\n",
      "        60s      1\n",
      "        saw      1\n",
      "    looking      1\n",
      "       mail      1\n",
      "      model      1\n",
      " production      1\n",
      "     really      1\n",
      "       rest      1\n",
      "   separate      1\n"
     ]
    }
   ],
   "source": [
    "text_index = 0\n",
    "tf_np_matrix = tf_matrix.toarray()\n",
    "tf_vector = tf_np_matrix[text_index]\n",
    "non_zero_indices = np.flatnonzero(tf_vector)\n",
    "unique_words = [words[index] for index in non_zero_indices]\n",
    "data = {'Word': unique_words,\n",
    "        'Count': tf_vector[non_zero_indices]}\n",
    "\n",
    "df = pd.DataFrame(data).sort_values('Count', ascending=False)\n",
    "print(f\"After stop-word deletion, {df.shape[0]} unique words remain.\")\n",
    "print(\"The 10 most frequent words are:\\n\")\n",
    "print(df[:10].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it’s worth noting that not all words are equal in their relevancy. Some words are more relevant to a car-discussion than others. For instance, the word model refers to a car-model (though of course it could also refer to a supermodel or a machine learning model). Meanwhile, the word really is more general. It doesn’t refer to anything car-related. The word is so irrelevant and common, that it could almost be a stop word. In fact, some NLP practitioners keep really on their stop-word list, while others don’t. Unfortunately, there is no shared consensus on which words are always useless, and which words aren’t. However, all practitioners agree that a word becomes less useful if its mentioned in too many texts. Thus, really is less relevant than model, because the former is mentioned more posts. Therefore, when ranking words by relevance, we should leverage both post-frequency and count. If two words share an equal count, then we should rank them by post-frequency instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets rerank our 34 words based on on both post-frequency and count. Afterwards, we’ll explore how these rankings can be used to improve text-vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< Screen z najczęściej wykorzystywanymi metodami Count Vectorizera>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking Words by Both Post-Frequency and Count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
