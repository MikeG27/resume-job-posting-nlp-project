{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Computing TF-IDF Vectors with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About dataset source\n",
    "\n",
    "Scikit-Learn provides us with data from Usenent, which is a well-established online collection of discussion forums. These Usenent forums are called newsgroups. Each individual newsgroup focuses on some topic of discussion. That discussion topic is briefly outlined within the newsgroup name. Users within a newsgroup converse by posting messages. These user posts are generally not limited in length. Some of the posts can get quite large. Both the diversity and the varying lengths of the posts will give us a chance to expand our NLP skills. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(remove=('headers','footers'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newsgroups object contains posts from 20 different newsgroups. Each newsgroup’s discussion-theme is outlined in its names. We can view these newsgroup names by printing newsgroups.target_names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, comp.sys.mac.hardware focuses on mac hardware. Meanwhile, comp.sys.ibm.pc_hardware focuses on pc hardware. Categorically, these 2 newsgroups are exceedingly similar. Their only differentiator is whether the computer hardware belongs to a mac or pc. Sometimes, categorical differences are subtle. Boundaries between text topics are fluid, and are not necessarily etched in stone. We’ll need to keep this in mind, later in the section, when we proceed to cluster the newsgroup posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get newsgroup group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "\n",
      "The post at index 1 first appeared in the 'comp.sys.mac.hardware' group\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "print(newsgroups.data[index])\n",
    "#get newsgroup\n",
    "origin = newsgroups.target_names[newsgroups.target[index]]\n",
    "print(f\"\\nThe post at index {index} first appeared in the '{origin}' group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get newsgroup data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our dataset contains 11314 newsgroup posts\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(newsgroups.data)\n",
    "print(f\"Our dataset contains {dataset_size} newsgroup posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains over 11,000 posts. **Our goal is to cluster these posts by topic.**\n",
    "Carrying out text clustering on this scale will require computational efficiency. We’ll need to efficiently compute newsgroup-post similarities by representing our text-data as a matrix. To do so, we’ll need to transform each newsgroup post into TF vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing Documents Using Scikit-Learn\n",
    "\n",
    "Scikit-Learn provides built-in class for transforming input texts into TF vectors. That class is called CountVectorizer. Initializing CountVectorizer will create a vectorizer object capable of vectorizing our texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need to do is run vectorizer.fit_transform(newsgroups.data). The method-call will return the TF matrix corresponding to the vectorized newsgroup posts. As a reminder, a TF matrix stores the counts of words (columns) across all texts (rows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remember \n",
    "\n",
    "TF Vector - Term Frequency Vector stores the counts of words(columns) across all texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 108644)\t4\n",
      "  (0, 110106)\t1\n",
      "  (0, 57577)\t2\n",
      "  (0, 24398)\t2\n",
      "  (0, 79534)\t1\n",
      "  (0, 100942)\t1\n",
      "  (0, 37154)\t1\n",
      "  (0, 45141)\t1\n",
      "  (0, 70570)\t1\n",
      "  (0, 78701)\t2\n",
      "  (0, 101084)\t4\n",
      "  (0, 32499)\t4\n",
      "  (0, 92157)\t1\n",
      "  (0, 100827)\t6\n",
      "  (0, 79461)\t1\n",
      "  (0, 39275)\t1\n",
      "  (0, 60326)\t2\n",
      "  (0, 42332)\t1\n",
      "  (0, 96432)\t1\n",
      "  (0, 67137)\t1\n",
      "  (0, 101732)\t1\n",
      "  (0, 27703)\t1\n",
      "  (0, 49871)\t2\n",
      "  (0, 65338)\t1\n",
      "  (0, 14106)\t1\n",
      "  :\t:\n",
      "  (11313, 55901)\t1\n",
      "  (11313, 93448)\t1\n",
      "  (11313, 97535)\t1\n",
      "  (11313, 93393)\t1\n",
      "  (11313, 109366)\t1\n",
      "  (11313, 102215)\t1\n",
      "  (11313, 29148)\t1\n",
      "  (11313, 26901)\t1\n",
      "  (11313, 94401)\t1\n",
      "  (11313, 89686)\t1\n",
      "  (11313, 80827)\t1\n",
      "  (11313, 72219)\t1\n",
      "  (11313, 32984)\t1\n",
      "  (11313, 82912)\t1\n",
      "  (11313, 99934)\t1\n",
      "  (11313, 96505)\t1\n",
      "  (11313, 72102)\t1\n",
      "  (11313, 32981)\t1\n",
      "  (11313, 82692)\t1\n",
      "  (11313, 101854)\t1\n",
      "  (11313, 66399)\t1\n",
      "  (11313, 63405)\t1\n",
      "  (11313, 61366)\t1\n",
      "  (11313, 7462)\t1\n",
      "  (11313, 109600)\t1\n"
     ]
    }
   ],
   "source": [
    "tf_matrix = vectorizer.fit_transform(newsgroups.data)\n",
    "print(tf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our printed tf_matrix does not appear to be a NumPy array.** What sort of data structure is it? We can check, by printing type(tf_matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tf_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About csr matrix \n",
    "\n",
    " stands for Compressed Sparse Row, which is a storage format for compressing matrices that are composed mostly of zeros. **These mostly empty matrices are referred to as sparse matrices. They can be made smaller by storing only the non-zero elements. This compression leads to more efficient memory usage, and also faster computation. Large-scale text-based matrices are usually very sparse, since a single document normally contains just a small percentage of the total vocabulary**. Thus, Scikit-Learn automatically converts the vectorized text to the CSR format. The conversion is carried out using a csr_matrix class that’s imported from SciPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to minimize confusion between csr_matrix and numpy we convert matrix to numpy array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "tf_np_matrix = tf_matrix.toarray()\n",
    "print(tf_np_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rows - representation of the post\n",
    "* Cols - represent individual words\n",
    "* Vocab_size = cols "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our collection of 11314 newsgroup posts contain a total of 114751 unique words\n"
     ]
    }
   ],
   "source": [
    "assert tf_np_matrix.shape == tf_matrix.shape\n",
    "num_posts, vocabulary_size = tf_np_matrix.shape\n",
    "print(f\"Our collection of {num_posts} newsgroup posts contain a total of \"\n",
    "      f\"{vocabulary_size} unique words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data contains 114,000 unique words. However, most posts will hold only a few dozen of these words. **We can measure the unique word-count of a post at index i by counting the number of non-zero elements in row tf_np_matrix[i].** The easiest way to count these non-zero elements is with NumPy. The library allows us to obtain all non-zero indices of the vector at tf_np_matrix[i]. We simply need to input the vector into the np.flatnonzero function. Below, we’ll count and output the non-zero indices of the car post in newsgroups.data[0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The newsgroup in row 0 contains 64 unique words.\n",
      "The actual word-counts map to the following column indices:\n",
      "\n",
      "[ 14106  15549  22088  23323  24398  27703  29357  30093  30629  32194\n",
      "  32305  32499  37154  39275  42332  42333  43643  45089  45141  49871\n",
      "  49881  50165  54442  55453  57577  58321  58842  60116  60326  64083\n",
      "  65338  67137  67140  68931  69080  70570  72915  75280  78264  78701\n",
      "  79055  79461  79534  82759  84398  87690  89161  92157  93304  95225\n",
      "  96145  96432 100406 100827 100942 101084 101732 108644 109086 109254\n",
      " 109294 110106 112936 113262]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "row_n = 0\n",
    "tf_vector = tf_np_matrix[row_n]\n",
    "non_zero_indices = np.flatnonzero(tf_vector)\n",
    "num_unique_words = non_zero_indices.size\n",
    "print(f\"The newsgroup in row {row_n} contains {num_unique_words} unique words.\")\n",
    "print(\"The actual word-counts map to the following column indices:\\n\")\n",
    "print(non_zero_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Okay cool... but what are these words ? \n",
    "\n",
    "In order to find out, we’ll need a mapping between TF vector indices and word-values. That mapping can be generated by calling vectorizer.get_feature_names(). The method-call will return a list of words, which we’ll call words. Each index i will correspond to the ith word within the list. Thus, running [words[i] for i in non_zero_indices] will return all unique words within our post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['60s', '70s', 'addition', 'all', 'anyone', 'be', 'body', 'bricklin', 'bumper', 'called', 'can', 'car', 'could', 'day', 'door', 'doors', 'early', 'engine', 'enlighten', 'from', 'front', 'funky', 'have', 'history', 'if', 'in', 'info', 'is', 'it', 'know', 'late', 'looked', 'looking', 'made', 'mail', 'me', 'model', 'name', 'of', 'on', 'or', 'other', 'out', 'please', 'production', 'really', 'rest', 'saw', 'separate', 'small', 'specs', 'sports', 'tellme', 'the', 'there', 'this', 'to', 'was', 'were', 'whatever', 'where', 'wondering', 'years', 'you']\n"
     ]
    }
   ],
   "source": [
    "words = vectorizer.get_feature_names()\n",
    "unique_words = [words[i] for i in non_zero_indices]\n",
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve printed all the words in newsgroups.data[0]. **Of course, not all these words have equal mention-counts. Some words occur more frequently than others. Perhaps these frequent words are more relevant to the topic of cars.** Lets print the 10 most frequent words within the post, along with their associated counts. We’ll represent this output as a Pandas table, for visualization purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Non-Zero Elements of 1D NumPy Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* non_zero_indices = np.flatnonzero(np_vector): Returns the non-zero indices in a 1D NumPy array.\n",
    "* non_zero_vector = np_vector[non_zero_indices]: Selects the non-zero elements of a 1D NumPy array (assuming non_zero_indices corresponds to non-zero indices of that array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Data\":unique_words,\n",
    "    \"Counts\":tf_vector[non_zero_indices]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>the</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>this</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>was</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>car</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>if</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>is</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>it</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>from</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>on</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anyone</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Data  Counts\n",
       "53     the       6\n",
       "55    this       4\n",
       "57     was       4\n",
       "11     car       4\n",
       "24      if       2\n",
       "27      is       2\n",
       "28      it       2\n",
       "19    from       2\n",
       "39      on       2\n",
       "4   anyone       2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data).sort_values(\"Counts\", ascending=False)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What a noise..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three first words, have nothing to do with cars. These words, the, this, and was, are among the most common words in the English language. They don’t provide a differentiating signal between the car-post and a differently-themed post.Instead, the common words are a source of noise. They increase the likelihood that 2 unrelated documents will cluster together.\n",
    "\n",
    "NLP practitioners refer to such noisy words as **stop words**,because these words are blocked from appearing in the vectorized results. Stop words are generally deleted from the text prior to vectorization. T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we’ll re-initialize a stop-word aware vectorizer. Afterwards, we’ll rerun fit_transfrom in order to re-compute the TF matrix. The number of word-columns in that matrix will be less than our previously computed vocabulary size of 114,751. Also, we’ll regenerate our words list. Common stop words such as the, this, of and it will be missing from that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "tf_matrix = vectorizer.fit_transform(newsgroups.data)\n",
    "assert tf_matrix.shape[1] < 114751\n",
    "\n",
    "words = vectorizer.get_feature_names()\n",
    "for common_word in ['the', 'this', 'was', 'if', 'it', 'on']:\n",
    "    assert common_word not in words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All stop words have been deleted from the recomputed tf_matrix. Now, we can re-generate the 10 most frequent words in newsgroups.data[0]. Please note that in the process, we’ll recompute tf_np_matrix, tf_vector, unique_words, non_zero_indices, and df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top words after stop-word deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After stop-word deletion, 34 unique words remain.\n",
      "The 10 most frequent words are:\n",
      "\n",
      "       Word  Count\n",
      "        car      4\n",
      "        60s      1\n",
      "        saw      1\n",
      "    looking      1\n",
      "       mail      1\n",
      "      model      1\n",
      " production      1\n",
      "     really      1\n",
      "       rest      1\n",
      "   separate      1\n"
     ]
    }
   ],
   "source": [
    "text_index = 0\n",
    "tf_np_matrix = tf_matrix.toarray()\n",
    "tf_vector = tf_np_matrix[text_index]\n",
    "non_zero_indices = np.flatnonzero(tf_vector)\n",
    "unique_words = [words[index] for index in non_zero_indices]\n",
    "data = {'Word': unique_words,\n",
    "        'Count': tf_vector[non_zero_indices]}\n",
    "\n",
    "df = pd.DataFrame(data).sort_values('Count', ascending=False)\n",
    "print(f\"After stop-word deletion, {df.shape[0]} unique words remain.\")\n",
    "print(\"The 10 most frequent words are:\\n\")\n",
    "print(df[:10].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it’s worth noting that not all words are equal in their relevancy. Some words are more relevant to a car-discussion than others. For instance, the word model refers to a car-model (though of course it could also refer to a supermodel or a machine learning model). Meanwhile, the word really is more general. It doesn’t refer to anything car-related. The word is so irrelevant and common, that it could almost be a stop word. In fact, some NLP practitioners keep really on their stop-word list, while others don’t. Unfortunately, there is no shared consensus on which words are always useless, and which words aren’t. However, all practitioners agree that a word becomes less useful if its mentioned in too many texts. Thus, really is less relevant than model, because the former is mentioned more posts. Therefore, when ranking words by relevance, we should leverage both post-frequency and count. If two words share an equal count, then we should rank them by post-frequency instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets rerank our 34 words based on on both post-frequency and count. Afterwards, we’ll explore how these rankings can be used to improve text-vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../../img/vectorization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking Words by Both Post-Frequency and Count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering matrix colummns with non_zero_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We obtained a sub-matrix correspond to the 34 words within post 0. The first row of the sub-matrix is:\n",
      "[1 1 1 1 1 1 1 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#row in this case is document\n",
    "row = 0\n",
    "sub_matrix = tf_np_matrix[:,non_zero_indices]\n",
    "print(\"We obtained a sub-matrix correspond to the 34 words within post 0. \"\n",
    "      \"The first row of the sub-matrix is:\")\n",
    "print(sub_matrix[row])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this moment we don't want to excract word-counts. Instead we just want to know whether each is present or not.\n",
    "In order to to this we transform our data to binary form.\n",
    "\n",
    "### Converting word counts to binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 1 1 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "binary_matrix = binarize(sub_matrix)\n",
    "print(binary_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This vector counts the unique posts in which each word is mentioned:\n",
      " [  18   21  202  314    4   26  802  536  842  154   67  348  184   25\n",
      "    7  368  469 3093  238  268  780  901  292   95 1493  407  354  158\n",
      "  574   95   98    2  295 1174]\n"
     ]
    }
   ],
   "source": [
    "unique_post_mentions = binary_matrix.sum(axis=0)\n",
    "print(\"This vector counts the unique posts in which each word is \"\n",
    "      f\"mentioned:\\n {unique_post_mentions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember ! \n",
    "\n",
    "* sum(axis=0) - vector of summed **rows**\n",
    "* sum(axis=1) - vector of summed **cols** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform counts into documents frequencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "      <th>Document Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>know</td>\n",
       "      <td>1</td>\n",
       "      <td>0.273378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>really</td>\n",
       "      <td>1</td>\n",
       "      <td>0.131960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>years</td>\n",
       "      <td>1</td>\n",
       "      <td>0.103765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Word  Count  Document Frequency\n",
       "17    know      1            0.273378\n",
       "24  really      1            0.131960\n",
       "33   years      1            0.103765"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_frequencies = unique_post_mentions / dataset_size\n",
    "data = {\"Word\": unique_words,\n",
    "       \"Count\": tf_vector[non_zero_indices],\n",
    "       \"Document Frequency\": document_frequencies}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df_common_words = df[df[\"Document Frequency\"] >= 0.1]\n",
    "df_common_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three of the 34 words appear have document frequency that’s greater than 0.1. As expected, these words are very general, and not car-specific. We thus can utilize document frequencies for ranking purposes. Lets rank our words by relevance, in the following manner. First, we’ll sort the word by count, from greatest to smallest. Afterwards, all words with equal count will be sorted by document frequency, from smallest to greatest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "      <th>Document Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>car</td>\n",
       "      <td>4</td>\n",
       "      <td>0.047375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>tellme</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bricklin</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>funky</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60s</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70s</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>enlighten</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bumper</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>doors</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>production</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>specs</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>sports</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>door</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>separate</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>engine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>addition</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>late</td>\n",
       "      <td>1</td>\n",
       "      <td>0.021036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>looked</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>model</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>wondering</td>\n",
       "      <td>1</td>\n",
       "      <td>0.026074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  Count  Document Frequency\n",
       "7          car      4            0.047375\n",
       "31      tellme      1            0.000177\n",
       "4     bricklin      1            0.000354\n",
       "14       funky      1            0.000619\n",
       "0          60s      1            0.001591\n",
       "1          70s      1            0.001856\n",
       "13   enlighten      1            0.002210\n",
       "5       bumper      1            0.002298\n",
       "10       doors      1            0.005922\n",
       "23  production      1            0.008397\n",
       "29       specs      1            0.008397\n",
       "30      sports      1            0.008662\n",
       "9         door      1            0.013611\n",
       "27    separate      1            0.013965\n",
       "12      engine      1            0.016263\n",
       "2     addition      1            0.017854\n",
       "18        late      1            0.021036\n",
       "19      looked      1            0.023687\n",
       "22       model      1            0.025809\n",
       "32   wondering      1            0.026074"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sorted = df.sort_values(['Count', 'Document Frequency'],\n",
    "                           ascending=[False, True])\n",
    "df_sorted.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sorting was successful. New car-related words, such as bumper, are now present in our list of top-ranked words. However, the actual sorting procedure was rather convoluted. It required us to sort 2 columns separately.**Perhaps, we can simplify the process by combining the word counts and document frequencies into a single score.** How can we combine these values? One approach is to divide each word-count by its associated document frequency. This resulting value will increase if:\n",
    "\n",
    "* The word-count goes up.\n",
    "* The document frequency goes down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_document_frequencies = 1 / document_frequencies\n",
    "df['IDF'] = inverse_document_frequencies\n",
    "df['Combined'] = df.Count * inverse_document_frequencies\n",
    "df_sorted = df.sort_values('Combined', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "      <th>Document Frequency</th>\n",
       "      <th>IDF</th>\n",
       "      <th>Combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>tellme</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>5657.000000</td>\n",
       "      <td>5657.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bricklin</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>2828.500000</td>\n",
       "      <td>2828.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>funky</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>1616.285714</td>\n",
       "      <td>1616.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60s</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>628.555556</td>\n",
       "      <td>628.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70s</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001856</td>\n",
       "      <td>538.761905</td>\n",
       "      <td>538.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>enlighten</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002210</td>\n",
       "      <td>452.560000</td>\n",
       "      <td>452.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bumper</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002298</td>\n",
       "      <td>435.153846</td>\n",
       "      <td>435.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>doors</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005922</td>\n",
       "      <td>168.865672</td>\n",
       "      <td>168.865672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>specs</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008397</td>\n",
       "      <td>119.094737</td>\n",
       "      <td>119.094737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>production</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008397</td>\n",
       "      <td>119.094737</td>\n",
       "      <td>119.094737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>sports</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008662</td>\n",
       "      <td>115.448980</td>\n",
       "      <td>115.448980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>car</td>\n",
       "      <td>4</td>\n",
       "      <td>0.047375</td>\n",
       "      <td>21.108209</td>\n",
       "      <td>84.432836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>door</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013611</td>\n",
       "      <td>73.467532</td>\n",
       "      <td>73.467532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>separate</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013965</td>\n",
       "      <td>71.607595</td>\n",
       "      <td>71.607595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>engine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016263</td>\n",
       "      <td>61.489130</td>\n",
       "      <td>61.489130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>addition</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017854</td>\n",
       "      <td>56.009901</td>\n",
       "      <td>56.009901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>late</td>\n",
       "      <td>1</td>\n",
       "      <td>0.021036</td>\n",
       "      <td>47.537815</td>\n",
       "      <td>47.537815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>looked</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023687</td>\n",
       "      <td>42.216418</td>\n",
       "      <td>42.216418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>model</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025809</td>\n",
       "      <td>38.746575</td>\n",
       "      <td>38.746575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>wondering</td>\n",
       "      <td>1</td>\n",
       "      <td>0.026074</td>\n",
       "      <td>38.352542</td>\n",
       "      <td>38.352542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>body</td>\n",
       "      <td>1</td>\n",
       "      <td>0.027753</td>\n",
       "      <td>36.031847</td>\n",
       "      <td>36.031847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>early</td>\n",
       "      <td>1</td>\n",
       "      <td>0.030758</td>\n",
       "      <td>32.511494</td>\n",
       "      <td>32.511494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>saw</td>\n",
       "      <td>1</td>\n",
       "      <td>0.031289</td>\n",
       "      <td>31.960452</td>\n",
       "      <td>31.960452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>history</td>\n",
       "      <td>1</td>\n",
       "      <td>0.032526</td>\n",
       "      <td>30.744565</td>\n",
       "      <td>30.744565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rest</td>\n",
       "      <td>1</td>\n",
       "      <td>0.035973</td>\n",
       "      <td>27.798526</td>\n",
       "      <td>27.798526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>info</td>\n",
       "      <td>1</td>\n",
       "      <td>0.041453</td>\n",
       "      <td>24.123667</td>\n",
       "      <td>24.123667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>small</td>\n",
       "      <td>1</td>\n",
       "      <td>0.050734</td>\n",
       "      <td>19.710801</td>\n",
       "      <td>19.710801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>looking</td>\n",
       "      <td>1</td>\n",
       "      <td>0.068941</td>\n",
       "      <td>14.505128</td>\n",
       "      <td>14.505128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>called</td>\n",
       "      <td>1</td>\n",
       "      <td>0.070886</td>\n",
       "      <td>14.107232</td>\n",
       "      <td>14.107232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>day</td>\n",
       "      <td>1</td>\n",
       "      <td>0.074421</td>\n",
       "      <td>13.437055</td>\n",
       "      <td>13.437055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>mail</td>\n",
       "      <td>1</td>\n",
       "      <td>0.079636</td>\n",
       "      <td>12.557159</td>\n",
       "      <td>12.557159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>years</td>\n",
       "      <td>1</td>\n",
       "      <td>0.103765</td>\n",
       "      <td>9.637138</td>\n",
       "      <td>9.637138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>really</td>\n",
       "      <td>1</td>\n",
       "      <td>0.131960</td>\n",
       "      <td>7.578031</td>\n",
       "      <td>7.578031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>know</td>\n",
       "      <td>1</td>\n",
       "      <td>0.273378</td>\n",
       "      <td>3.657937</td>\n",
       "      <td>3.657937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  Count  Document Frequency          IDF     Combined\n",
       "31      tellme      1            0.000177  5657.000000  5657.000000\n",
       "4     bricklin      1            0.000354  2828.500000  2828.500000\n",
       "14       funky      1            0.000619  1616.285714  1616.285714\n",
       "0          60s      1            0.001591   628.555556   628.555556\n",
       "1          70s      1            0.001856   538.761905   538.761905\n",
       "13   enlighten      1            0.002210   452.560000   452.560000\n",
       "5       bumper      1            0.002298   435.153846   435.153846\n",
       "10       doors      1            0.005922   168.865672   168.865672\n",
       "29       specs      1            0.008397   119.094737   119.094737\n",
       "23  production      1            0.008397   119.094737   119.094737\n",
       "30      sports      1            0.008662   115.448980   115.448980\n",
       "7          car      4            0.047375    21.108209    84.432836\n",
       "9         door      1            0.013611    73.467532    73.467532\n",
       "27    separate      1            0.013965    71.607595    71.607595\n",
       "12      engine      1            0.016263    61.489130    61.489130\n",
       "2     addition      1            0.017854    56.009901    56.009901\n",
       "18        late      1            0.021036    47.537815    47.537815\n",
       "19      looked      1            0.023687    42.216418    42.216418\n",
       "22       model      1            0.025809    38.746575    38.746575\n",
       "32   wondering      1            0.026074    38.352542    38.352542\n",
       "3         body      1            0.027753    36.031847    36.031847\n",
       "11       early      1            0.030758    32.511494    32.511494\n",
       "26         saw      1            0.031289    31.960452    31.960452\n",
       "15     history      1            0.032526    30.744565    30.744565\n",
       "25        rest      1            0.035973    27.798526    27.798526\n",
       "16        info      1            0.041453    24.123667    24.123667\n",
       "28       small      1            0.050734    19.710801    19.710801\n",
       "20     looking      1            0.068941    14.505128    14.505128\n",
       "6       called      1            0.070886    14.107232    14.107232\n",
       "8          day      1            0.074421    13.437055    13.437055\n",
       "21        mail      1            0.079636    12.557159    12.557159\n",
       "33       years      1            0.103765     9.637138     9.637138\n",
       "24      really      1            0.131960     7.578031     7.578031\n",
       "17        know      1            0.273378     3.657937     3.657937"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new ranking has failed! The word car no longer appears at the top of the list. What happened? Well, lets take a look at our table. There is a problem with the IDF values: some of them are huge! The printed IDF values range from approximately 100 to over 5000. Meanwhile, our word-count range is very small. The counts vary from 1 to 4. Thus, when we multiply word-counts by IDF values, the IDF will dominate. The counts will then have no impact on the final results. We need to somehow make our IDF values smaller. What should we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Math solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data scientists are commonly confronted with numeric values that are too large. One way to shrink the values down is to apply a logarithmic function. For instance, running np.log10(1000000) will return 6. Essentially, a value of 1,000,000 will be replaced by the count of zeros in that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Combined'] = df.Count * np.log10(df.IDF)\n",
    "df_sorted = df.sort_values('Combined', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "      <th>Document Frequency</th>\n",
       "      <th>IDF</th>\n",
       "      <th>Combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>car</td>\n",
       "      <td>4</td>\n",
       "      <td>0.047375</td>\n",
       "      <td>21.108209</td>\n",
       "      <td>5.297806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>tellme</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>5657.000000</td>\n",
       "      <td>3.752586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bricklin</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>2828.500000</td>\n",
       "      <td>3.451556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>funky</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>1616.285714</td>\n",
       "      <td>3.208518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60s</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>628.555556</td>\n",
       "      <td>2.798344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70s</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001856</td>\n",
       "      <td>538.761905</td>\n",
       "      <td>2.731397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>enlighten</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002210</td>\n",
       "      <td>452.560000</td>\n",
       "      <td>2.655676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bumper</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002298</td>\n",
       "      <td>435.153846</td>\n",
       "      <td>2.638643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>doors</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005922</td>\n",
       "      <td>168.865672</td>\n",
       "      <td>2.227541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>specs</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008397</td>\n",
       "      <td>119.094737</td>\n",
       "      <td>2.075893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>production</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008397</td>\n",
       "      <td>119.094737</td>\n",
       "      <td>2.075893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>sports</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008662</td>\n",
       "      <td>115.448980</td>\n",
       "      <td>2.062390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>door</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013611</td>\n",
       "      <td>73.467532</td>\n",
       "      <td>1.866095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>separate</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013965</td>\n",
       "      <td>71.607595</td>\n",
       "      <td>1.854959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>engine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016263</td>\n",
       "      <td>61.489130</td>\n",
       "      <td>1.788798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  Count  Document Frequency          IDF  Combined\n",
       "7          car      4            0.047375    21.108209  5.297806\n",
       "31      tellme      1            0.000177  5657.000000  3.752586\n",
       "4     bricklin      1            0.000354  2828.500000  3.451556\n",
       "14       funky      1            0.000619  1616.285714  3.208518\n",
       "0          60s      1            0.001591   628.555556  2.798344\n",
       "1          70s      1            0.001856   538.761905  2.731397\n",
       "13   enlighten      1            0.002210   452.560000  2.655676\n",
       "5       bumper      1            0.002298   435.153846  2.638643\n",
       "10       doors      1            0.005922   168.865672  2.227541\n",
       "29       specs      1            0.008397   119.094737  2.075893\n",
       "23  production      1            0.008397   119.094737  2.075893\n",
       "30      sports      1            0.008662   115.448980  2.062390\n",
       "9         door      1            0.013611    73.467532  1.866095\n",
       "27    separate      1            0.013965    71.607595  1.854959\n",
       "12      engine      1            0.016263    61.489130  1.788798"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sorted.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TFIDF is a simple but powerful metric for ranking words within a document. Of course, the metric is only relevant if that document is part of a larger document group. Otherwise, the computed TFIDF values all equal zero.** Also, the metric loses its effectiveness when applied to small collections of similar tests. Nonetheless, for most real-world text datasets, TFIDF produces good ranking results. Furthermore, the metric has additional uses. It can be utilized to vectorize words within a document. The numeric content of df.Combined is essentially a vector. It was produced by modifying the TF vector stored in df.Count. In this same manner, we can transform any TF vector into a TFIDF vector. We just need to multiply the TF vector by the log of inverse document frequencies.\n",
    "\n",
    "Is there a benefit to transforming TF vectors into more complicated TFIDF vectors? Yes! Within larger text datasets, TFIDF vectors provide a greater signal of textual similarity and divergence. For example, 2 texts that are both discussing cars are more likely to cluster together if their irrelevant vector elements are penalized. Thus, penalizing common words using the IDF will improve the clustering of large text collections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This isn’t necessarily true of smaller datasets, where the number of documents is low, and the document frequency is high. Consequently, the IDF might be too small to meaningfully improve the clustering results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computin TFIDF Vectors with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That TfidfVectorizer class is nearly identical to CountVectorizer, except that it takes IDF into account during the vectorization process. Below, we’ll import TfidfVectorizer from sklearn.feature_extraction.text. Afterwards, we’ll initialize the class by running TfidfVectorizer(stop_words=’english). The constructed tfidf_vectorizer object will be parametrized to ignore all stop-words. Subsequently, executing tfidf_vectorizer.fit_transform(newsgroups.data) will return a matrix of vectorized TFIDF values. The matrix-shape will be identical to tf_matrix.shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(newsgroups.data)\n",
    "assert tfidf_matrix.shape == tf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tfdif_vectorizer has learned the same vocabulary as the simpler TF vectorizer. In fact, the indices of words in tfidf_matrix are identical to those of tf_matrix. We can confirm this by calling tfidf_vectorizer.get_feature_names(). The method-call will return an ordered list of words that is identical to our previously computed words list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tfidf_vectorizer.get_feature_names() == words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since word-order is preserved, we should expect the non-zero indices of tfidf_matrix[0] to equal our previously computed non_zero_indices array. We’ll confirm below, after converting tfidf_matrix from a CSR data-structure to a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_np_matrix = tfidf_matrix.toarray()\n",
    "tfidf_vector = tfidf_np_matrix[0]\n",
    "tfidf_non_zero_indices = np.flatnonzero(tfidf_vector)\n",
    "assert np.array_equal(tfidf_non_zero_indices,\n",
    "                      non_zero_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-zero indices of tf_vector and tfidif_vector are identical. We thus can add the TFIDF vector as a column in our existing df table. Adding a TFIDF column will allow us to compare Scikit-Learn’s output with our manually-computed score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TFIDF'] = tfidf_vector[non_zero_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "      <th>Document Frequency</th>\n",
       "      <th>IDF</th>\n",
       "      <th>Combined</th>\n",
       "      <th>TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>car</td>\n",
       "      <td>4</td>\n",
       "      <td>0.047375</td>\n",
       "      <td>21.108209</td>\n",
       "      <td>5.297806</td>\n",
       "      <td>0.459552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>tellme</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>5657.000000</td>\n",
       "      <td>3.752586</td>\n",
       "      <td>0.262118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bricklin</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>2828.500000</td>\n",
       "      <td>3.451556</td>\n",
       "      <td>0.247619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>funky</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>1616.285714</td>\n",
       "      <td>3.208518</td>\n",
       "      <td>0.234280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60s</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>628.555556</td>\n",
       "      <td>2.798344</td>\n",
       "      <td>0.209729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word  Count  Document Frequency          IDF  Combined     TFIDF\n",
       "7        car      4            0.047375    21.108209  5.297806  0.459552\n",
       "31    tellme      1            0.000177  5657.000000  3.752586  0.262118\n",
       "4   bricklin      1            0.000354  2828.500000  3.451556  0.247619\n",
       "14     funky      1            0.000619  1616.285714  3.208518  0.234280\n",
       "0        60s      1            0.001591   628.555556  2.798344  0.209729"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(\"TFIDF\",ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting by df.TFIDF should produce a relevance ranking that is consistent with our previous observations. Lets verify that both df.TFIDF and df.Combined produce the same word-rankings after sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Word  Count  Document Frequency          IDF  Combined     TFIDF\n",
      "       car      4            0.047375    21.108209  5.297806  0.459552\n",
      "    tellme      1            0.000177  5657.000000  3.752586  0.262118\n",
      "  bricklin      1            0.000354  2828.500000  3.451556  0.247619\n",
      "     funky      1            0.000619  1616.285714  3.208518  0.234280\n",
      "       60s      1            0.001591   628.555556  2.798344  0.209729\n",
      "       70s      1            0.001856   538.761905  2.731397  0.205568\n",
      " enlighten      1            0.002210   452.560000  2.655676  0.200827\n",
      "    bumper      1            0.002298   435.153846  2.638643  0.199756\n",
      "     doors      1            0.005922   168.865672  2.227541  0.173540\n",
      "     specs      1            0.008397   119.094737  2.075893  0.163752\n"
     ]
    }
   ],
   "source": [
    "df_sorted_old = df.sort_values('Combined', ascending=False)\n",
    "df_sorted_new = df.sort_values('TFIDF', ascending=False)\n",
    "assert np.array_equal(df_sorted_old['Word'].values,\n",
    "                      df_sorted_new['Word'].values)\n",
    "print(df_sorted_new[:10].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our word-rankings have remained unchanged. However, the values of the TFIDF and Combined columns are not identical. Our top 10 manually-computed Combined values are all greater than 1. Meanwhile, all of Scikit-Learn’s TFIDF values are less than 1. Why is this the case?\n",
    "\n",
    "**As it turns out, Scikit-Learn automatically normalizes its TFIDF vector results. The magnitude of df.TFIDF has been modified to equal 1. We can confirm by calling norm(df.TFIDF.values).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "assert norm(df.TFIDF.values) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would Scikit-Learn automatically normalize the vectors? For our own benefit! As discussed in Section Thirteen, its easier to compute text-vector similarity when all vector magnitudes equal 1. Consequently, our normalized TFIDF matrix is primed for similarity analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../../img/Vectorizer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute TF-IDF\n",
    "\n",
    "1. Use CountVectorizer with stopwords to counts words in documents\n",
    "2. For code simplicity convert asr matrix to numpy\n",
    "3. Compute word frequencies across documents\n",
    "4. Compute inverse word frequencies (1/freq)\n",
    "5. Combine the word counts and document frequencies into a single score (Count * inverse transform)\n",
    "6. Obtain TF - IDF by log(point5)\n",
    "\n",
    "or... \n",
    "\n",
    "Use TfidfVectorizer ands it's all !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next \n",
    "\n",
    "Read about bag of words because TF-IDF solve drawbacks of bag of words technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember \n",
    "\n",
    "* TF - how many times a word is used in that entire document\n",
    "* IDF - how important is the word in the entire list of documents \n",
    "\n",
    "So using TF and IDF machine makes sense of important words in a document and important words throughout all documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF\n",
    "\n",
    "https://medium.com/analytics-vidhya/tf-idf-term-frequency-technique-easiest-explanation-for-text-classification-in-nlp-with-code-8ca3912e58c3\n",
    "\n",
    "Bag of words\n",
    "\n",
    "https://medium.com/swlh/bag-of-words-code-the-easiest-explanation-of-nlp-technique-using-a-python-8a4fdfb8598c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
