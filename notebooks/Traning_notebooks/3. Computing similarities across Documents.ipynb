{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Similarities Across Large Documents Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important question : Which of our Newsgroup post is most similar to first one? \n",
    "\n",
    "We can obtain the answer by computing all the cosine similarities between **tfidf_np_matrix** and **tf_np_matrix[0]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(remove=('headers','footers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(newsgroups.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get first post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_np_matrix = tfidf_matrix.toarray()\n",
    "tfidf_vector = tfidf_np_matrix[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute similarities\n",
    "\n",
    "As we remember from previous Data Science Bookcamp section 13 we can obtain vector similarities with dot.product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.00834093 0.04448717 ... 0.         0.00270615 0.01968562]\n",
      "11314\n"
     ]
    }
   ],
   "source": [
    "cosine_similarities = tfidf_np_matrix @ tfidf_np_matrix[0]\n",
    "print(cosine_similarities)\n",
    "print(len(cosine_similarities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its output is a vector of cosine similarities. Each ith index of the vector corresponds to the cosine similarity between newsgroups.data[0] and newsgroups.data[i]. From the print-out, we can see that cosine_similarities[0] is equal to 1.0. This is not surprising, since newsgroups_data[0] will have a perfect similarity with itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the next highest similarity in vector ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar index is...958\n"
     ]
    }
   ],
   "source": [
    "most_similar_index = np.argsort(cosine_similarities)[-2]\n",
    "print(f\"Most similar index is...{most_similar_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest similarity indicator is ...0.6410493167298943\n"
     ]
    }
   ],
   "source": [
    "#Get similarity level by index\n",
    "similarity = cosine_similarities[most_similar_index]\n",
    "print(f\"Highest similarity indicator is ...{similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following post has a cosine similarity of 0.64 with newsgroups.data[0]:\n",
      "\n",
      "In article <1993Apr20.174246.14375@wam.umd.edu> lerxst@wam.umd.edu (where's my  \n",
      "thing) writes:\n",
      "> \n",
      ">  I was wondering if anyone out there could enlighten me on this car I saw\n",
      "> the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "> early 70s. It was called a Bricklin. The doors were really small. In  \n",
      "addition,\n",
      "> the front bumper was separate from the rest of the body. This is \n",
      "> all I know. If anyone can tellme a model name, engine specs, years\n",
      "> of production, where this car is made, history, or whatever info you\n",
      "> have on this funky looking car, please e-mail.\n",
      "\n",
      "Bricklins were manufactured in the 70s with engines from Ford. They are rather  \n",
      "odd looking with the encased front bumper. There aren't a lot of them around,  \n",
      "but Hemmings (Motor News) ususally has ten or so listed. Basically, they are a  \n",
      "performance Ford with new styling slapped on top.\n",
      "\n",
      ">    ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "Rush fan?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get most similar post \n",
    "most_similar_post = newsgroups.data[most_similar_index]\n",
    "print(f\"The following post has a cosine similarity of {similarity:.2f} \"\n",
    "       \"with newsgroups.data[0]:\\n\")\n",
    "print(most_similar_post)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The printed text is... a reply to the car-post at index 0!** Reply includes the original post, which is a question about the certain car-brand.\n",
    "\n",
    "Their cosine similarity is 0.64. This does not seem like a large number. However, within extensive text collections, a cosine similarity that’s greater than 0.6 is good indicator of overlapping content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note ! \n",
    "\n",
    "As discussed in Section Thirteen, the cosine similarity can easily be converted into the Tanimoto similarity, which has a deeper theoretical basis for text overlap. We can convert cosine_similarities into Tanimoto similarities by running cosine_similarities / (2 - cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanimoto_similarities(documents,text):\n",
    "    cosine_similarity = documents @ text\n",
    "    tanimoto_similarity = cosine_similarity / (2 - cosine_similarity)\n",
    "    return tanimoto_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "958"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(tanimoto_similarities(tfidf_np_matrix,tfidf_np_matrix[0]))[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " However, that conversion will not change our final results. Choosing the top index of the Tanimoto array will still return the same posted reply. Thus, for simplicity’s sake, we will focus on the cosine similarity during our next few text-comparison examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "1. Pick post at random and then choose its most similar neigbour.\n",
    "2. Output both post along with their cosine similary\n",
    "\n",
    "In order to make this exercise more interesting, we'll first compute a matrix of all-by-all cosine similarities. We'll then laverage the matrix to select our random pair of similar posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we compute the matrix of all-by-all cosine similarities?\n",
    "\n",
    "The naive situation is to multiple **tfidf_np_matrix** with its transpose. However, for reasons discussed in Section thirteen the **matrix multiplication is not computationally efficient**. \n",
    "\n",
    "Out TFIDF matrix has over 100 000 columns. **We need to reduce the matrix size, prior to executing the multiplication**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TruncatedSVD \n",
    "\n",
    "In previous section we learng how to reduce the column-count using scikit-learn **TruncatedSVD** class. \n",
    "\n",
    "The class is able to shrint a matrix down to a specified number of columns. The reduced column-count is determined by the n_components parameter. \n",
    "\n",
    "According to Scikit-Learn's documentation an n_components **value of 100 is recommended for processing text data**.\n",
    "\n",
    "Documentation : https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've dimensionally-reduced a 114441-column <class 'scipy.sparse.csr.csr_matrix'> matrix.\n",
      "Our output is a 100-column <class 'numpy.ndarray'> matrix.\n"
     ]
    }
   ],
   "source": [
    "#set pseudo-random generator\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "shrunk_matrix = TruncatedSVD(n_components=100).fit_transform(tfidf_matrix)\n",
    "print(f\"We've dimensionally-reduced a {tfidf_matrix.shape[1]}-column \"\n",
    "      f\"{type(tfidf_matrix)} matrix.\")\n",
    "print(f\"Our output is a {shrunk_matrix.shape[1]}-column \"\n",
    "      f\"{type(shrunk_matrix)} matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our shrunk matrix contains just 100 columns. We can now efficient compute cosine similarities by running **shrunk_matrix @ shrunk_matrix.T.**\n",
    "\n",
    "**However, first we’ll need to confirm that the matrix rows remain normalized.** Lets check the magnitude of shrunk_matrix[0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The magnitude of the first row is 0.49\n"
     ]
    }
   ],
   "source": [
    "magnitude = np.linalg.norm(shrunk_matrix[0])\n",
    "print(f\"The magnitude of the first row is {magnitude:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magnitude of the row is less than 1. Scikit-Learn’s SVD output has not been automatically normalized. We’ll need to manually normalize the matrix, prior to computing the similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The magnitude of the first row is 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "shrunk_norm_matrix = normalize(shrunk_matrix)\n",
    "magnitude = np.linalg.norm(shrunk_norm_matrix[0])\n",
    "print(f\"The magnitude of the first row is {magnitude:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shrunken matrix has been normalized. Now, running shrunk_norm_matrix @ shrunk_norm_matrix.T should produce a matrix of all-by-all cosine similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  4.20207330e-02,  2.09103735e-01, ...,\n",
       "        -3.48442355e-02, -6.79152644e-04,  1.95435814e-01],\n",
       "       [ 4.20207330e-02,  1.00000000e+00,  3.01777775e-01, ...,\n",
       "         5.44917724e-01,  4.54122181e-02,  1.67038105e-01],\n",
       "       [ 2.09103735e-01,  3.01777775e-01,  1.00000000e+00, ...,\n",
       "         2.40456903e-01,  8.10512396e-02,  1.07226785e-01],\n",
       "       ...,\n",
       "       [-3.48442355e-02,  5.44917724e-01,  2.40456903e-01, ...,\n",
       "         1.00000000e+00,  1.92546180e-02,  5.53444235e-02],\n",
       "       [-6.79152644e-04,  4.54122181e-02,  8.10512396e-02, ...,\n",
       "         1.92546180e-02,  1.00000000e+00,  8.38745651e-02],\n",
       "       [ 1.95435814e-01,  1.67038105e-01,  1.07226785e-01, ...,\n",
       "         5.53444235e-02,  8.38745651e-02,  1.00000000e+00]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_matrix = shrunk_norm_matrix @ shrunk_norm_matrix.T\n",
    "cosine_similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have our similarity matrix !\n",
    "\n",
    "Lets leverage it to choose a random pair of very similar texts. We’ll start by randomly selecting a post at some index1. We’ll next select an index of cosine_similarities[index1], that has the second-highest cosine similarity. Then, we’ll print both the indices and their similarity prior to displaying the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The posts at indices 1146 and 5558 share a cosine similarity of 0.98\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(4)\n",
    "index1 = np.random.randint(dataset_size)\n",
    "index2 = np.argsort(cosine_similarity_matrix[index1])[-2]\n",
    "similarity = cosine_similarity_matrix[index1][index2]\n",
    "print(f\"The posts at indices {index1} and {index2} share a cosine \"\n",
    "      f\"similarity of {similarity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usually when I start up an application, I first get the window outline\n",
      "on my display. I then have to click on the mouse button to actually\n",
      "place the window on the screen. Yet when I specify the -geometry \n",
      "option the window appears right away, the properties specified by\n",
      "the -geometry argument. The question now is:\n",
      "How can I override the intermediary step of the user having to specify\n",
      "window position with a mouseclick? I've tried explicitly setting window\n",
      "size and position, but that did alter the normal program behaviour.\n",
      "Thanks for any hints\n",
      "---> Robert\n",
      "PS: I'm working in plain X.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups.data[index2].replace('\\n\\n', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I posted this about tow weeks ago but never saw it make it (Then again\n",
      "I've had some problems with the mail system). Apologies if this appears\n",
      "for the second time:\n",
      "Usually when I start up an application, I first get the window outline\n",
      "on my display. I then have to click on the mouse button to actually\n",
      "place the window on the screen. Yet when I specify the -geometry \n",
      "option the window appears right away, the properties specified by\n",
      "the -geometry argument. The question now is:\n",
      "How can I override the intermediary step of the user having to specify\n",
      "window position with a mouseclick? I've tried explicitly setting window\n",
      "size and position, but that did alter the normal program behaviour.\n",
      "Thanks for any hints\n",
      "---> Robert\n",
      "PS: I'm working in plain X, using tvtwm.\n",
      "\n",
      "******************************************************************************\n",
      "* Robert Gasch        * Der erste Mai ist der Tag an dem die Stadt ins      *\n",
      "* Oracle Engineering   * Freihe tritt und den staatlichen Monopolanspruch    *\n",
      "* De Meern, NL        * auf Gewalt in Frage stellt                          *\n",
      "* rgasch@nl.oracle.com *                           - Einstuerzende Neubauten *\n",
      "******************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups.data[index1].replace('\\n\\n', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far, we have examined 2 pairs of similar posts. Each post-pair was composed of a question and a reply, where the question was included in the reply. Such boring pairs of overlapping texts are trivial to extract. Lets challenge ourselves to find something more interesting.\n",
    "\n",
    "We’ll search for clusters of similar texts, where posts within a cluster share some text without perfectly overlapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
